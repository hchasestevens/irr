\documentclass[a4paper,11pt]{proposal}
\usepackage{times}  %% This makes the body base font times...
\usepackage{url}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath, amsthm}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage[olditem]{paralist}
\usepackage{epsfig}


\setlength{\headheight}{14pt} %Fixes headheight warning
\widowpenalty=1000
\clubpenalty=1000



% Put in your details here
\title{Natural Language Programming}
\crest{\includegraphics[width=40mm]{crest.pdf}}
\author{H. Chase Stevens}
\studentnumber{s1107496}
% For IRR
\collegeordept{School of Informatics}
% For IRP
%\collegeordept{Supervisor: Dr. M. Supervisor}


\university{University of Edinburgh}

% For IRR
\degree{Informatics Research Review}
% For IRP
%\degree{M.Sc. Project Proposal}


\degreedate{January 2015} 



\begin{document}
\maketitle 
\parindent=0mm


\pagenumbering{roman}
\setcounter{tocdepth}{2}

\clearpage
\abstract{
The aim of natural language programming is to allow written natural language
specifications to be transformed into corresponding formal language representations executable by computer. Research in this field has largely tackled this task as performed in restricted domains; focusing on restricted subsets of both natural language specifications and programs. Three domains in particular have received special attention in the literature, these being the use of formalized, restricted variants of natural language specifications for the creation of programs, the translation of unrestricted natural language descriptions or directives into imperative formal languages, and the translation of natural language specifications or questions into declarative formal languages. There are also numerous methodologies and techniques used in natural language programming, the most prominent being those used in the field of machine translation, including semantic parsing, tree transduction, and, most recently, translation via artificial neural network.
}

\newpage

\pagenumbering{arabic}

\setlength{\parskip}{1ex} 


\section{Introduction} \label{sec:intro}

Traditionally, the execution of commands on a computer has either required the user to express their intentions through a formalized language or to use a specialized tool or software application to achieve the same effect. This has provided a natural motivation for investigations into the use of natural languages, in which humans can easily express intentions, as a replacement for or interface to machine-executable languages. This goal has become known as ``natural language programming". 

Natural language programming has had a storied history. While early results in using natural language for querying databases showed some promise, their practical application was heavily limited by the use of pre-statistical techniques which rendered them tightly domain-bound and fragile. With the rise to prominence of statistical techniques in the broader field of natural language processing in tandem with the availability of larger and larger corpora for training, more recent work in natural language programming has surpassed the limitations of earlier systems and is making substantive inroads toward natural language/formal language translation, especially in restricted domains. 

The creation of more robust natural language programming systems not only has the potential to allow non-technical computer users a greater level of control over their devices, but could also offer improvements to traditional software development methodologies, mirroring the successes of using machine translation techniques to assist human natural language translation. Further, a sufficiently advanced natural language programming system could plausibly extract machine-executable commands from sources not explicitly intended as computer directives, such as descriptions of algorithms or even ``how-to" articles.

This review will present both a historical perspective on the issue of natural language programming and an overview of the current state-of-the-art techniques, while also positing potential directions for further research.



\section{Background} \label{sec:back}
Broadly, approaches to natural language programming can be split into two categories: those that attempt to make formal languages more closely resemble natural language, and those that attempt to handle unrestricted natural language expressions. While the former approach has been largely abandoned in favor of the latter within the more recent literature, an overview is given here of both types, in order not only to establish a historical context for the problem of natural language programming but also to provide sufficient juxtaposition to enable a comparison of the approaches' relative strengths and weaknesses. The latter approach, in turn, can be subdivided into those approaches targeting domain-specific declarative languages (often query languages) and those which target more general imperative languages.

\subsection{Formalized natural language subsets}

Under the ``formalized natural language subset" paradigm, rather than a natural language text being translated into a corresponding formal language representation, a formal language itself is designed to, as much as possible, resemble and capture some subset of natural language. As such, expressions written in systems using this approach are directly parsed and executed, without having to undergo any other processing. In general, this approach shares many traits with other pre-statistical natural language processing methodologies; systems employing this approach are tightly bound to specific domains, such as database querying and processing, and can be quite fragile. 

A prominent example of such a system is ``Natural Language Computer" (``NLC"), which is designed for processing data stored in tables \cite{ballard1979} \cite{biermann1983}. The system allows for the description of procedures which query and manipulate these data through English-language commands, for example, ``Choose a row in the matrix" or ``Add five [to] the 2nd positive entry in col 2" \cite{ballard1979}. NLC is emblematic of a ``good old-fashioned AI" approach in that its lexicon consists solely of terms explicitly pre-defined. This means that the system is only able to recognize a very small vocabulary of 450 words and misspellings thereof \cite{ballard1979}. A hand-written grammar is used to parse natural language statements, and while this approach in conjunction with the limited vocabulary produces good translations of the recognized subset of English, it also severely limits what can be legally expressed (for example, both declarative statements and questions are disallowed) and presents a clear issue in terms of the system's scalability. Nevertheless, experimental results \cite{biermann1983} testify that NLC is perceived by users as providing a means of performing data manipulation that is both easier and faster than its contemporary alternatives. On the other hand, results also demonstrate that a significant proportion of users failed to achieve desired results using NLC, and also highlight users' frustrations in having to re-formulate English-language expressions in order to target NLC's subset of English.

The ``ROBOT" system employs a similar methodology to NLC, but with some notable distinctions, namely, that ROBOT allows both imperative commands (``PRINT THE NAME, MODEL OF CAR, AND INSURANCE COMPANY OF ALL EMPLOYEES") and questions (``WHAT DID WE SELL CUSTOMER 24618?") to be used, and that ROBOT leverages data labels (e.g. column names) from the database to be queried, incorporating these labels into its lexicon \cite{harris1977}. It should be noted, however, that the latter is not an automatic process, but requires mappings between lexemes and data labels or values to be entered manually by database administrators. While users' success rate with ROBOT differs little from NLC, the system has the advantage of not requiring users to know the structure of tables in order to query them.

Unlike both NLC and ROBOT, the unnamed system presented in Heidorn \cite{heidorn1974} allows for declarative English-language specifications of scenarios for simulation. While the overall approach used is similar to that of NLC, including a limited hand-written lexicon, it notably differs in offering an interactive interface. In this way, the user is allowed to deliver a specification, then is asked by the system to clarify, elaborate on, and correct the system's interpretation of this specification. Despite the use of interactive corrective components in this and other systems having been noted as being an important feature for users \cite{capindale1990}, very little more recent work has incorporated this,  perhaps because more modern statistical or translation-based approaches do not lend themselves as easily to interactive prompts.

There are several issues that pervade early natural language subset approaches to database querying, as highlighted in Capindale and Crawford \cite{capindale1990}. The permitted vocabulary is often tightly bound to a specific database, which both serves to limit the usefulness and applicability of the natural language interface and requires users to have an intimate knowledge of the targeted database. Without being given a formal specification of the grammar and lexicon, users also often have to re-formulate queries repeatedly in order to achieve the results they want. 

Indeed, the specificity with which users have to formulate their queries or directives, coupled with the disadvantage that even a carefully worded sentence may fail to succeed, raises the question of whether these systems as a whole are more effective than well designed formal query languages (such as SQL), alternative database management UIs, or other more traditional approaches. This is the case with the both NLC and the system presented in Heidorn \cite{heidorn1974}, in which a full specification of the problem resembling a more verbose version of what might alternatively be expressed in source code is required from the user: while the user is spared having to learn a formal language's syntax, they instead are forced to use English to unambiguously explicate a scenario in a high level of detail.

\subsection{Approaches targeting declarative formal languages}
%\begin{itemize}
%\item Maybe split into 3 subsections - query languages (SQL, XPath), %functional (e.g. ROBOT, string manipulation), regular expressions (are these %just a special case of query?) - problem: some papers (ge, mooney) span %multiple?
%\end{itemize}

The brunt of recent work in the literature has focused on using statistical methods to translate natural language into declarative, domain-specific formal languages. These efforts have in particular been aided by the release of the Geoquery \cite{zelle1995} and Robocup \cite{kuhlmann2004} corpora, which map English language expressions (``What are the major cities in Kansas?", ``If player 2 has the ball, player 2 should pass to player 10") to respective formal representations as Prolog queries and CLang directives.

Operating on the assumption that suitable syntactic parsers are available for the natural language descriptions, several techniques have been identified which have made substantive improvements to natural-language/formal-language translations for these data sets. For example, approaches in which mappings are learned from subtrees of the NL syntatic parse to subtrees of the equivalent formal-language representation have been able to improve on both the speed and quality of translation \cite{kate2005}. This approach, however, does rely on the assumption that direct subtree-subtree translation without context from surrounding parts of the natural language parse is sufficient. The subtree mapping technique also opens up the possibility for bi-directional natural-language/source code translation. In particular, methods using tree transduction on learned subtree correspondences have demonstrated results on the Geoquery dataset that exceed baseline translation efforts, although have not yet reached state-of-the-art levels of performance \cite{karampatsis2015}.

In an alternative approach, restrictions on the vocabulary allowed for natural language descriptions can be leveraged to provide even greater advances in translation quality. The technique assumes one or more lambda-calculus representations exist for each terminal and non-terminal node in the natural language syntactic tree, which itself is assumed to be binarized. These representations could either be hand-made or, in principle, induced from a corpus. The lambda-calculus representations can then be composed into a number of possible resultant expressions; supervised learning is used to identify features that are most likely to indicate correct semantic representations \cite{ge2009}.

Similar restrictions on vocabulary have been leveraged to drive techniques for taking natural-language query specifications and converting them into a derivative of XPath, allowing for the querying of X/HTML documents \cite{tannier2005}. In a manner somewhat reminiscent of earlier pre-statistical approaches, the problem is tackled from two sides: the XPath derivative used includes high-level predicates (e.g. ``\texttt{definition(a)}", ``\texttt{about(a)}") so as to be more clearly derivable from the natural language descriptions than standard XPath; likewise, the system only allows the use of a restricted set of lexical items which directly correspond to these high-level predicates, making parsing and subsequent semantic conversion an easier task. Improved results are achieved by using techniques from information retrieval to evaluate the relevance of various generated formal-language queries' results to the original natural-language query. The leveraging of domain-specific operations in this method both serves to produce impressive results but also limits the technique's applicability to cases where high-level general procedures have already been defined in the target language.

String-based querying and manipulation through natural language specifications has also been tackled. In particular, there have been recent successes in generating regular expressions from natural language descriptions, e.g. ``three letter word starting with ’X’". To achieve this, a semantic categorical grammar which targets the lambda calculus is induced, then employed. Individual features of the description, once parsed, can then be unified into a single expression, which is then translatable into a regular expression \cite{kushman2013}.

More recent work in string querying and manipulation uses natural language descriptions of desired functionality in combination with user-provided examples to create string query and manipulation programs as formalized using a declarative DSL. By requiring the user to specify both positive and negative examples not only for the overall task, but also individual constituents of the natural language description provided, improved results can be achieved. For example, given an input like ``Replace any combination of 6 whole numbers with `X'," the user would be able to provide examples of ``6 whole numbers". By using these examples for particular phrases within the natural language specification, the search space of possible programs adhering to the description is greatly constrained. In doing so, improvements are made to the accuracy both with which sub-components of the program are mapped to description constituents and with which these sub-components are composed together to reflect the overall program description \cite{raza2015}.

%\begin{itemize}
%\item Need to determine if ``Semantic Parsing on Freebase from Question-Answer Pairs" \cite{berant2013} should be included here or not.
%\end{itemize}


\subsection{Approaches targeting imperative formal languages}

Less work has seemingly been done in selecting imperative languages as targets for natural language programming, despite their frequent use in the practice of software development. Preliminary results of a system which generates simple programs (in C) from descriptions such as ``find the maximum and minimum numbers" have, however, been presented \cite{mou2015}. The system as presented is a recurrent neural network, which reads input and generates programs on a character-by-character (not token-by-token) basis. This approach has the disadvantage that the source code generated is not guaranteed to be syntactically valid, but it appears that the programs generated using this technique require only a few manual corrections to the code produced in to successfully perform as expected.

Less promising have been attempts to convert user-provided natural language descriptions of procedures into non-executable program "skeletons", which can then be edited into working programs by the user. One particular technique attempts to find three types of features within the descriptions: "steps", "loops", and "comments". Shallow syntactic parses are used to identify these features and specify in what way they should be transformed into the skeleton \cite{mihalcea2006}. This technique appears to be relatively brittle and has the disadvantage of requiring human intervention to produce executable code, however, given the successes in machine-assisted translation, further work in this area has the potential to achieve favorable results.

\section{Discussion and Conclusion} \label{sec:disc}
%\begin{itemize}
%\item Comparison of relative strengths/weaknesses of various approaches
%\end{itemize}

Overall, it is clear that the natural language programming literature is trending toward more tightly-focused systems which leverage modern statistical techniques, and in doing so demonstrate improved performance when compared to more general, rudimentary systems. The field is now in a good position to capitalize upon recent advances made in the related subjects of automatic programming, machine translation, and machine learning.

In particular, there are several notable areas in which more research is clearly warranted. The literature reveals relatively little exploration of the use of artificial neural networks for natural language programming, despite repeated successes in traditional machine translation tasks (\cite{bahdanau2014}, \cite{sutskever2014}). 
The investigation of whether recent advances in neural network training techniques and architecture design - such as the incorporation of an external stack, which has proven to be of benefit in other natural language processing tasks \cite{das1992} - might successfully be applied to the problem of natural language programming seems very likely to bear fruit. Specifically, the application of recurrent neural networks, which are well suited for use with tree-structured data, to natural language programming tasks appears to be a natural next step.

Further, translation efforts for generalized natural language descriptions have largely targeted more conventional imperative languages (such as C++ \cite{mou2015}) despite greater success in more declarative languages, such as CLang \cite{kate2005} and GeoQuery \cite{wong2006}. It's unclear what properties if any have driven the successes in the latter: two possible explanations are that either the inherently stateful nature of the imperative paradigm poses a challenge, or that declarative languages may tend toward simpler grammars. Regardless, this suggests an opportunity to attempt targeting a concise but non-domain specific functional language.

A particular difficulty for natural language programming is the lack of large sets of training data, one notable exception being the recently released ``If This Then That" (IFTTT) corpus \cite{quirk2015}. Without these, modern statistical approaches are severely limited in effectiveness. Further, what corpora are available do not offer parallel translations of natural language descriptions to multiple formal languages, but instead target a single formal language (e.g. the GeoQuery corpus). Likewise, corpora appear to be available largely for only a single natural language - English - which impedes work on natural language programming in other languages. A potential solution for this would be to create a corpus by generating natural language descriptions of code - a conceivably easier task, and one that has been previously tackled in the literature \cite{sridhara2010}. Alternatively, function or method comments written for publicly accessible codebases online could be leveraged as natural language descriptions of the code they accompany, provided these comments can be trusted to describe the code to a sufficient level of detail and accuracy.



\newpage
\bibliographystyle{IEEEtran}
\bibliography{irr_irp}
\end{document}
